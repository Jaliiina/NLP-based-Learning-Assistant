import os
import tempfile

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
import matplotlib
import matplotlib.pyplot as plt
import re

from matplotlib import font_manager

# ===== å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šå­—ä½“æ–‡ä»¶ï¼ˆä¸æ”¹ä»»ä½•ä¸šåŠ¡åŠŸèƒ½ï¼‰=====
FONT_PATH = "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc"

if os.path.exists(FONT_PATH):
    try:
        # æ³¨å†Œå­—ä½“åˆ° matplotlib
        font_manager.fontManager.addfont(FONT_PATH)

        # è·å–å­—ä½“åï¼Œå¹¶è®¾ä¸ºå…¨å±€é»˜è®¤å­—ä½“
        _font_name = font_manager.FontProperties(fname=FONT_PATH).get_name()
        matplotlib.rcParams["font.family"] = _font_name

        # è´Ÿå·æ­£å¸¸æ˜¾ç¤º
        matplotlib.rcParams["axes.unicode_minus"] = False
    except Exception:
        # å…œåº•ï¼šå³ä½¿æ³¨å†Œå¤±è´¥ï¼Œä¹Ÿåˆ«è®©ç¨‹åºå´©
        matplotlib.rcParams["axes.unicode_minus"] = False
else:
    # å­—ä½“æ–‡ä»¶ä¸å­˜åœ¨å°±åˆ«ç¡¬è®¾ï¼Œé¿å…è¿è¡Œæ—¶æŠ¥é”™
    matplotlib.rcParams["axes.unicode_minus"] = False

try:
    import jieba  

    JIEBA_AVAILABLE = True
except Exception:
    jieba = None
    JIEBA_AVAILABLE = False

try:
    from gensim.models import KeyedVectors, Word2Vec  

    GENSIM_AVAILABLE = True
except Exception:
    KeyedVectors = None
    Word2Vec = None
    GENSIM_AVAILABLE = False


def load_stopwords():
    stopwords_path = os.path.join(os.path.dirname(__file__), "stopwords.txt")
    stopwords = set()
    if os.path.exists(stopwords_path):
        try:
            with open(stopwords_path, "r", encoding="utf-8") as f:
                for line in f:
                    word = line.strip()
                    if word:
                        stopwords.add(word)
        except Exception:
            pass
    return stopwords


def split_paragraphs(text: str):
    if not text:
        return []
    raw_paras = text.replace("\r\n", "\n").split("\n\n")
    paras = [p.strip() for p in raw_paras if p.strip()]
    return paras


def tokenize_for_tfidf(paragraphs, use_jieba: bool, stopwords):
    processed = []
    for p in paragraphs:
        if use_jieba and JIEBA_AVAILABLE and jieba is not None:
            tokens = jieba.lcut(p)
        else:
            tokens = p.split()
        tokens = [t for t in tokens if t not in stopwords]
        processed.append(" ".join(tokens))
    return processed


def tokenize_sentences_for_w2v(text: str, use_jieba: bool, stopwords):
    sentences = []
    for line in text.replace("\r\n", "\n").split("\n"):
        line = line.strip()
        if not line:
            continue
        if use_jieba and JIEBA_AVAILABLE and jieba is not None:
            tokens = jieba.lcut(line)
        else:
            tokens = line.split()
        tokens = [t for t in tokens if t not in stopwords]
        if tokens:
            sentences.append(tokens)
    return sentences


def tfidf_page():
    st.header("ğŸ“ˆ æ®µè½å…³é”®è¯åˆ†æ")

    st.markdown(
        """
<div style="background-color:#f8fafc;padding:20px;border-radius:16px;border:1px solid #e2e8f0;
box-shadow:0 4px 16px rgba(0,0,0,0.06);margin:16px 0 24px 0;">
  <div style="font-size:16px;color:#1e293b;line-height:1.75;">
    è¿™ä¸ªé¡µé¢ç”¨æ¥åšä¸¤ä»¶äº‹ï¼š<br/>
    <span style="color:#2563eb;font-weight:700;">â‘  ç»™æ¯ä¸ªæ®µè½æå–å…³é”®è¯</span>ï¼ˆçœ‹å®Œå°±çŸ¥é“è¿™ä¸€æ®µåœ¨è®²ä»€ä¹ˆï¼‰<br/>
    <span style="color:#2563eb;font-weight:700;">â‘¡ æ‰¾å‡ºæœ€â€œä¿¡æ¯å¯†åº¦é«˜â€çš„é‡ç‚¹æ®µè½</span>ï¼ˆä¼˜å…ˆå¤ä¹ /åšç¬”è®°ï¼‰
  </div>
  <div style="margin-top:16px;background:#eff6ff;padding:12px;border-radius:10px;color:#1e40af;">
    ğŸ§­ <b>ä½ ä¼šåšä»€ä¹ˆï¼š</b>ç²˜è´´è®²ä¹‰ï¼ˆç©ºè¡Œåˆ†æ®µï¼‰â†’ é€‰æ‹©ä¸€ä¸ªæ®µè½ â†’ çœ‹å…³é”®è¯æ¡å½¢å›¾ â†’ çœ‹é‡ç‚¹æ®µè½æ’è¡Œ<br/>
    <span style="opacity:0.9;">ï¼ˆå†…éƒ¨ç”¨çš„æ˜¯ TF-IDF æƒé‡ï¼šç”¨æ¥è¡¡é‡â€œè¿™ä¸ªè¯å¯¹å½“å‰æ®µè½æœ‰å¤šé‡è¦â€ã€‚ï¼‰</span>
  </div>
</div>
""",
        unsafe_allow_html=True,
    )

    if "shared_text" not in st.session_state:
        st.session_state["shared_text"] = ""

    raw_text = st.text_area(
        "ğŸ“„ è¾“å…¥è®²ä¹‰æ–‡æœ¬ï¼ˆå»ºè®®ç”¨ç©ºè¡Œåˆ†æ®µï¼‰",
        height=260,
        key="shared_text",
        help=(
            "æç¤ºï¼šç”¨ç©ºè¡Œï¼ˆ\n\nï¼‰åˆ†éš”æ®µè½æ•ˆæœæœ€å¥½ã€‚ä½ åœ¨è¿™é‡Œçš„ç¼–è¾‘åªå½±å“æœ¬é¡µè®¡ç®—ï¼Œä¸ä¼šå½±å“å…¶ä»–é¡µé¢ã€‚"
        ),
    )

    st.subheader("âš™ï¸ å¯é€‰è®¾ç½®")
    col1, col2, col3 = st.columns(3)

    with col1:
        ngram_max = st.slider(
            "æœ€å¤§ n-gram é•¿åº¦",
            min_value=1,
            max_value=3,
            value=1,
            step=1,
            help="n-gram=2/3 ä¼šæŠŠç›¸é‚»è¯æ‹¼æˆçŸ­è¯­ç‰¹å¾ï¼ˆæ›´åƒâ€œå…³é”®çŸ­è¯­â€ï¼‰ã€‚",
        )
    with col2:
        max_features = st.slider(
            "ç‰¹å¾æ•°é‡ä¸Šé™ max_features",
            min_value=100,
            max_value=5000,
            value=1000,
            step=100,
            help="ç‰¹å¾è¶Šå¤šè¶Šç»†è‡´ï¼Œä½†è®¡ç®—ä¹Ÿæ›´æ…¢ï¼›1000 é€šå¸¸å¤Ÿç”¨ã€‚",
        )
    with col3:
        use_jieba = st.checkbox("ä¸­æ–‡åˆ†è¯ï¼ˆjiebaï¼‰", value=False, help="ä¸­æ–‡å»ºè®®å¼€å¯ï¼›è‹±æ–‡æˆ–å·²åˆ†è¯æ–‡æœ¬å¯å…³é—­ã€‚")

    paragraphs = split_paragraphs(raw_text)
    if not paragraphs:
        st.info("è¯·åœ¨ä¸Šæ–¹è¾“å…¥è‡³å°‘ä¸€ä¸ªæ®µè½ï¼ˆå»ºè®®ç”¨ç©ºè¡Œåˆ†éš”æ®µè½ï¼‰ã€‚")
        return

    stopwords = load_stopwords()
    processed_paragraphs = tokenize_for_tfidf(paragraphs, use_jieba, stopwords)

    try:
        vectorizer = TfidfVectorizer(
            ngram_range=(1, ngram_max),
            max_features=max_features,
        )
        tfidf_matrix = vectorizer.fit_transform(processed_paragraphs)
        feature_names = vectorizer.get_feature_names_out()
        st.session_state["tfidf_paragraphs"] = paragraphs
        st.session_state["tfidf_matrix"] = tfidf_matrix
        st.session_state["tfidf_features"] = feature_names
        st.session_state["tfidf_scores"] = tfidf_matrix.sum(axis=1).A1
    except Exception as e:
        st.error(f"TF-IDF è®¡ç®—å‡ºé”™ï¼š{e}")
        return

    st.subheader("ğŸ“¦ æ®µè½åˆ‡åˆ†ç»“æœ")
    st.write(f"å·²è¯†åˆ« **{len(paragraphs)}** ä¸ªæ®µè½ï¼ˆæŒ‰ç©ºè¡Œåˆ‡åˆ†ï¼‰ã€‚")
    st.caption(f"å½“å‰ç‰¹å¾æ•°ï¼š{len(feature_names)}ï¼ˆç”¨äºè®¡ç®—å…³é”®è¯ï¼›æ— éœ€åˆ»æ„è¿½æ±‚è¶Šå¤§è¶Šå¥½ï¼‰")

    st.markdown("### ğŸ” 1) é€‰ä¸­ä¸€ä¸ªæ®µè½ â†’ è‡ªåŠ¨æå–å…³é”®è¯")
    st.caption("ä½ ä¼šå¾—åˆ°ï¼šå…³é”®è¯è¡¨æ ¼ + å½©è‰²æ¡å½¢å›¾ï¼ˆè¶Šé•¿è¶Šé‡è¦ï¼‰ã€‚")

    para_index = st.selectbox(
        "é€‰æ‹©ä¸€ä¸ªæ®µè½ï¼š",
        options=list(range(len(paragraphs))),
        format_func=lambda i: f"ç¬¬ {i + 1} æ®µï¼š" + (paragraphs[i][:40] + ("..." if len(paragraphs[i]) > 40 else "")),
    )

    top_k = st.slider("æ˜¾ç¤ºæƒé‡æœ€é«˜çš„å‰ K ä¸ªè¯/çŸ­è¯­", min_value=5, max_value=30, value=10, step=1)

    row = tfidf_matrix[para_index].toarray().flatten()
    if row.sum() == 0:
        st.info("æ‰€é€‰æ®µè½çš„ TF-IDF æƒé‡å…¨ä¸º 0ï¼Œå¯èƒ½æ˜¯å› ä¸ºåˆ†è¯æˆ–åœç”¨è¯è¿‡æ»¤å¯¼è‡´ã€‚")
    else:
        top_indices = row.argsort()[::-1][:top_k]
        data = [(feature_names[i], float(row[i])) for i in top_indices if row[i] > 0]
        if not data:
            st.info("æœªæ‰¾åˆ°æƒé‡å¤§äº 0 çš„ç‰¹å¾è¯ã€‚")
        else:
            df_top = pd.DataFrame(data, columns=["è¯/çŸ­è¯­", "TF-IDF æƒé‡"])
            st.table(df_top)

            fig, ax = plt.subplots(figsize=(9.5, 4.8))
            labels = list(reversed(df_top["è¯/çŸ­è¯­"].tolist()))
            values = list(reversed(df_top["TF-IDF æƒé‡"].tolist()))
            colors = plt.cm.viridis(np.linspace(0.2, 0.95, len(values)))
            ax.barh(labels, values, color=colors, edgecolor="none")
            ax.set_title("æ®µè½å…³é”®è¯æ¡å½¢å›¾ï¼ˆTF-IDFï¼‰", pad=10)
            ax.set_xlabel("TF-IDF æƒé‡")
            ax.grid(axis="x", linestyle="--", alpha=0.25)
            for spine in ["top", "right"]:
                ax.spines[spine].set_visible(False)
            fig.tight_layout()
            st.pyplot(fig)

    st.markdown("---")
    st.markdown("### ğŸ† 2) æ‰¾é‡ç‚¹æ®µè½ï¼ˆä¼˜å…ˆå¤ä¹ /åšç¬”è®°ï¼‰")
    st.caption("æ®µè½æ€»åˆ†è¶Šé«˜ï¼Œé€šå¸¸ä»£è¡¨ï¼šä¿¡æ¯æ›´å¯†ã€æœ¯è¯­æ›´é›†ä¸­ã€‚å¯ä»¥æŠŠå®ƒä»¬å½“ä½œâ€œé‡ç‚¹æ®µâ€ã€‚")

    doc_scores = tfidf_matrix.sum(axis=1).A1

    if len(paragraphs) == 1:
        st.info("å½“å‰åªæœ‰ 1 ä¸ªæ®µè½ï¼Œæ— æ³•è¿›è¡Œæ®µè½é—´å¯¹æ¯”ã€‚")
    else:
        max_top_n = min(10, len(paragraphs))
        top_n = st.slider(
            "æ˜¾ç¤ºå‰ N ä¸ªé‡éš¾ç‚¹æ®µè½",
            min_value=1,
            max_value=max_top_n,
            value=min(3, max_top_n),
            step=1,
        )

        top_idx = doc_scores.argsort()[::-1][:top_n]
        df_rank = pd.DataFrame(
            [(int(i) + 1, float(doc_scores[i]), paragraphs[i][:80]) for i in top_idx],
            columns=["æ®µè½ç¼–å·", "TF-IDF æ€»åˆ†", "æ®µè½é¢„è§ˆ"],
        )
        st.table(df_rank)

        fig2, ax2 = plt.subplots(figsize=(9.5, 4.2))
        y = [str(int(i) + 1) for i in top_idx[::-1]]
        x = [float(doc_scores[i]) for i in top_idx[::-1]]
        colors2 = plt.cm.plasma(np.linspace(0.15, 0.9, len(x)))
        ax2.barh(y, x, color=colors2, edgecolor="none")
        ax2.set_title("é‡ç‚¹æ®µè½æ’è¡Œï¼ˆæŒ‰ TF-IDF æ€»åˆ†ï¼‰", pad=10)
        ax2.set_xlabel("æ®µè½ TF-IDF æ€»åˆ†")
        ax2.set_ylabel("æ®µè½ç¼–å·")
        ax2.grid(axis="x", linestyle="--", alpha=0.25)
        for spine in ["top", "right"]:
            ax2.spines[spine].set_visible(False)
        fig2.tight_layout()
        st.pyplot(fig2)

    st.markdown("---")
    st.info("âœ… å·²å®Œæˆï¼šå…³é”®è¯æå– + é‡ç‚¹æ®µè½æ’åºã€‚ä½ å¯ä»¥å›åˆ°ä¸Šæ–¹æ¢æ®µè½ï¼Œæˆ–è°ƒæ•´å‚æ•°é‡æ–°è§‚å¯Ÿå˜åŒ–ã€‚")


def load_pretrained_w2v(uploaded_file):
    if not GENSIM_AVAILABLE or KeyedVectors is None:
        return None

    suffix = os.path.splitext(uploaded_file.name)[1].lower()

    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(uploaded_file.getvalue())
        tmp_path = tmp.name

    try:
        if suffix == ".model" and Word2Vec is not None:
            model = Word2Vec.load(tmp_path)
        else:
            model = KeyedVectors.load_word2vec_format(tmp_path, binary=True)
        return model
    except Exception as e:
        st.error(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥ï¼š{e}")
        return None
    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass


def train_w2v_from_text(text: str, use_jieba: bool, vector_size: int, window: int, min_count: int, epochs: int):
    if not GENSIM_AVAILABLE or Word2Vec is None:
        return None

    stopwords = load_stopwords()
    sentences = tokenize_sentences_for_w2v(text, use_jieba, stopwords)

    if not sentences:
        st.warning("è¯­æ–™ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚")
        return None

    model = Word2Vec(
        sentences=sentences,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=2,
        epochs=epochs,
    )
    return model


def visualize_embeddings(model, method: str, n_words: int):
    words = model.wv.index_to_key[:n_words]
    vectors = np.array([model.wv[w] for w in words])

    if vectors.shape[0] < 3:
        st.warning("è¯è¡¨å¤ªå°ï¼Œæ— æ³•å¯è§†åŒ–ã€‚")
        return

    if method == "PCA":
        reducer = PCA(n_components=2)
        coords = reducer.fit_transform(vectors)
    else:
        perplexity = min(30, max(5, vectors.shape[0] // 3))
        reducer = TSNE(
            n_components=2,
            perplexity=perplexity,
            learning_rate=200.0,
            init="random",
            max_iter=1000,
        )
        coords = reducer.fit_transform(vectors)

    t = np.linspace(0, 1, len(words))
    sizes = 20 + 220 * (1 - t) ** 0.8

    fig, ax = plt.subplots(figsize=(9.5, 6))
    sc = ax.scatter(
        coords[:, 0],
        coords[:, 1],
        c=t,
        cmap="Spectral",
        s=sizes,
        alpha=0.88,
        edgecolors="white",
        linewidths=0.6,
    )

    for i, word in enumerate(words):
        ax.annotate(
            word,
            (coords[i, 0], coords[i, 1]),
            fontsize=8,
            alpha=0.9,
            color="#0f172a",
            bbox=dict(boxstyle="round,pad=0.18", fc="white", ec="none", alpha=0.5),
        )

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f"Word2Vec æœ¯è¯­è¯­ä¹‰ç©ºé—´ï¼ˆ{method} é™ç»´ï¼›é¢œè‰²=è¯é¢‘æ’åï¼‰", pad=12)
    ax.grid(True, linestyle="--", alpha=0.18)
    for spine in ["top", "right", "left", "bottom"]:
        ax.spines[spine].set_visible(False)
    cbar = fig.colorbar(sc, ax=ax, fraction=0.046, pad=0.02)
    cbar.set_label("è¯é¢‘æ’åï¼ˆå½’ä¸€åŒ–ï¼‰")
    fig.tight_layout()
    st.pyplot(fig)


def word2vec_page():
    st.header("ğŸ§  è¯­ä¹‰ç†è§£ä¸æ¦‚å¿µå…³è”")

    st.markdown(
        """
<div style="background-color:#f8fafc;padding:20px;border-radius:16px;border:1px solid #e2e8f0;
box-shadow:0 4px 16px rgba(0,0,0,0.06);margin:16px 0 24px 0;">
  <div style="font-size:16px;color:#1e293b;line-height:1.75;">
    è¿™ä¸ªé¡µé¢ç”¨æ¥åšä¸‰ä»¶äº‹ï¼š<br/>
    <span style="color:#2563eb;font-weight:700;">â‘  å‡†å¤‡ä¸€ä¸ªè¯å‘é‡æ¨¡å‹</span>ï¼ˆåŠ è½½é¢„è®­ç»ƒ or ç”¨ä½ çš„è®²ä¹‰è®­ç»ƒä¸€ä¸ªï¼‰<br/>
    <span style="color:#2563eb;font-weight:700;">â‘¡ è¾“å…¥æœ¯è¯­ï¼ŒæŸ¥çœ‹â€œå®ƒæœ€åƒè°â€</span>ï¼ˆå¿«é€Ÿæ‰¾ç›¸å…³æ¦‚å¿µï¼‰<br/>
    <span style="color:#2563eb;font-weight:700;">â‘¢ ç”»ä¸€å¼ æœ¯è¯­åœ°å›¾</span>ï¼ˆçœ‹çœ‹æ¦‚å¿µæ˜¯å¦ä¼šè‡ªåŠ¨èšæˆç°‡ï¼‰
  </div>
  <div style="margin-top:16px;background:#eff6ff;padding:12px;border-radius:10px;color:#1e40af;">
    ğŸ§­ <b>ä½ ä¼šåšä»€ä¹ˆï¼š</b>åŠ è½½/è®­ç»ƒæ¨¡å‹ â†’ è¾“å…¥è¯æŸ¥è¯¢ç›¸ä¼¼è¯ â†’ ä¸€é”®å¯è§†åŒ–ï¼ˆPCA/t-SNEï¼‰
  </div>
</div>
""",
        unsafe_allow_html=True,
    )

    if not GENSIM_AVAILABLE:
        st.error("å½“å‰ç¯å¢ƒæœªå®‰è£… gensimï¼Œæ— æ³•è¿›è¡Œ Word2Vec å®éªŒã€‚")
        return

    st.subheader("ğŸ§© ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æ¨¡å‹")
    st.caption("å¦‚æœä½ åªæ˜¯æƒ³ä½“éªŒï¼šç”¨ä¸€ä¸ªå°è¯­æ–™è‡ªè®­å°±å¤Ÿäº†ï¼›å¦‚æœä½ æœ‰ç°æˆæ¨¡å‹æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ç›´æ¥åŠ è½½ã€‚")

    mode = st.radio(
        "æ¨¡å‹æ¥æº",
        ["ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹", "ä»æ–‡æœ¬è‡ªè®­æ¨¡å‹"],
        help="é¢„è®­ç»ƒæ¨¡å‹ï¼šé€‚åˆç›´æ¥æŸ¥è¯¢ï¼›è‡ªè®­æ¨¡å‹ï¼šé€‚åˆç”¨ä½ çš„è®²ä¹‰æ¢ç´¢è¯¾ç¨‹æœ¯è¯­å…³ç³»ã€‚",
    )
    use_jieba = st.checkbox("ä¸­æ–‡åˆ†è¯ï¼ˆjiebaï¼‰", value=False, help="ä¸­æ–‡è®­ç»ƒ/æŸ¥è¯¢å»ºè®®å¼€å¯ã€‚")

    model = st.session_state.get("w2v_model", None)

    if mode == "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹":
        st.subheader("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
        uploaded = st.file_uploader(
            "ä¸Šä¼  Word2Vec é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒ gensim .model æˆ– word2vec .binï¼‰",
            type=["bin", "model"],
        )
        if uploaded is not None and st.button("ğŸ“¦ åŠ è½½æ¨¡å‹", type="primary"):
            with st.spinner("æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹..."):
                model = load_pretrained_w2v(uploaded)
                if model is not None:
                    st.session_state["w2v_model"] = model
                    st.success("é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    else:
        st.subheader("ğŸ§ª ç”¨è¯¾ç¨‹è¯­æ–™è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹")

        demo_corpus = (
            "è‡ªç„¶è¯­è¨€å¤„ç† æ˜¯ äººå·¥æ™ºèƒ½ çš„ é‡è¦ åˆ†æ”¯ ã€‚\n"
            "è¯å‘é‡ æ¨¡å‹ å¯ä»¥ å°† å•è¯ æ˜ å°„ åˆ° è¿ç»­ å‘é‡ ç©ºé—´ ã€‚\n"
            "å­¦ç”Ÿ å¯ä»¥ é€šè¿‡ äº¤äº’å¼ å®éªŒ åŠ æ·± å¯¹ NLP æ¦‚å¿µ çš„ ç†è§£ ã€‚"
        )

        if "w2v_corpus_text" not in st.session_state:
            seed = st.session_state.get("shared_text", "")
            st.session_state["w2v_corpus_text"] = seed if isinstance(seed, str) and seed.strip() else demo_corpus

        raw_text = st.text_area(
            "è®­ç»ƒè¯­æ–™ï¼ˆå»ºè®®ï¼šæ¯è¡Œä¸€å¥ï¼›è‹¥å·²åˆ†è¯åˆ™ä»¥ç©ºæ ¼åˆ†éš”ï¼‰",
            height=200,
            key="w2v_corpus_text",
            help="Word2Vec çš„è®­ç»ƒå•å…ƒæ˜¯â€œå¥å­â€ã€‚æ¯è¡Œä¸€å¥æ›´ç¨³å®šï¼›æ®µè½ä¹Ÿå¯ä»¥ï¼Œä½†å»ºè®®å…ˆæ–­å¥ã€‚",
        )

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            vector_size = st.slider("å‘é‡ç»´åº¦", 50, 300, 100, step=50)
        with col2:
            window = st.slider("çª—å£å¤§å° window", 2, 10, 5, step=1)
        with col3:
            min_count = st.slider("æœ€å°è¯é¢‘ min_count", 1, 5, 1, step=1)
        with col4:
            epochs = st.slider("è®­ç»ƒè½®æ•° epochs", 5, 50, 10, step=5)

        if st.button("ğŸ§ª å¼€å§‹è®­ç»ƒæ¨¡å‹", type="primary"):
            with st.spinner("æ­£åœ¨è®­ç»ƒ Word2Vec æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                model = train_w2v_from_text(
                    text=raw_text,
                    use_jieba=use_jieba,
                    vector_size=vector_size,
                    window=window,
                    min_count=min_count,
                    epochs=epochs,
                )
                if model is not None:
                    st.session_state["w2v_model"] = model
                    st.success("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    if model is None:
        st.info("è¯·å…ˆåœ¨ä¸Šæ–¹åŠ è½½æˆ–è®­ç»ƒä¸€ä¸ª Word2Vec æ¨¡å‹ã€‚")
        return

    st.markdown("---")
    st.subheader("ğŸ” ç¬¬äºŒæ­¥ï¼šæŸ¥ç›¸ä¼¼æœ¯è¯­")
    st.caption("è¾“å…¥ä¸€ä¸ªè¯ï¼Œæ¨¡å‹ä¼šè¿”å›å®ƒæœ€æ¥è¿‘çš„ Top-N è¯ï¼ˆå¯ä»¥å½“ä½œâ€œç›¸å…³æ¦‚å¿µæ¨èâ€ï¼‰ã€‚")

    query_word = st.text_input("è¾“å…¥ä¸€ä¸ªæœ¯è¯­/è¯æ±‡ï¼š", help="ä¼šè¿”å›æœ€ç›¸ä¼¼çš„ Top-N æœ¯è¯­åŠç›¸ä¼¼åº¦ã€‚")
    topn = st.slider("æ˜¾ç¤ºå‰ N ä¸ªç›¸ä¼¼æœ¯è¯­", min_value=5, max_value=30, value=10, step=1)

    if query_word:
        try:
            similar = model.wv.most_similar(query_word, topn=topn)
            df_sim = pd.DataFrame(similar, columns=["ç›¸ä¼¼æœ¯è¯­", "ç›¸ä¼¼åº¦"])
            st.table(df_sim)
        except KeyError:
            st.warning("è¯¥æœ¯è¯­ä¸åœ¨å½“å‰æ¨¡å‹çš„è¯è¡¨ä¸­ï¼Œè¯·å°è¯•å…¶ä»–è¯æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚")
        except Exception as e:
            st.error(f"è®¡ç®—ç›¸ä¼¼æœ¯è¯­æ—¶å‡ºé”™ï¼š{e}")

    st.markdown("---")
    st.subheader("ğŸ¨ ç¬¬ä¸‰æ­¥ï¼šç”»ä¸€å¼ æœ¯è¯­åœ°å›¾")
    st.caption("æ¯ä¸ªç‚¹æ˜¯ä¸€ä¸ªè¯ï¼šè¶Šè¿‘è¶Šåƒã€‚é¢œè‰²è¡¨ç¤ºè¯é¢‘æ’åï¼ˆè¶Šå¸¸è§é¢œè‰²è¶Šé å‰ï¼‰ã€‚")

    vocab_size = len(model.wv.index_to_key)
    if vocab_size == 0:
        st.info("å½“å‰æ¨¡å‹è¯è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¯è§†åŒ–ã€‚")
        return

    max_words = min(200, vocab_size)
    n_words = st.slider(
        "é€‰æ‹©è¦å¯è§†åŒ–çš„æœ¯è¯­æ•°é‡ï¼ˆæŒ‰é¢‘ç‡æ’åå‰ N ä¸ªï¼‰",
        min_value=10,
        max_value=max_words,
        value=min(50, max_words),
        step=10,
    )

    method = st.radio("é™ç»´æ–¹æ³•", ["PCA", "t-SNE"], horizontal=True)

    if st.button("ğŸ¨ å¼€å§‹å¯è§†åŒ–", type="primary"):
        with st.spinner("æ­£åœ¨è®¡ç®—é™ç»´åæ ‡å¹¶ç»˜å›¾..."):
            visualize_embeddings(model, method, n_words)
